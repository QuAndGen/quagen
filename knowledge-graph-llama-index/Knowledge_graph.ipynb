{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Libraries"
      ],
      "metadata": {
        "id": "kl5ieat8UzwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "\n",
        "# !pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface transformers accelerate bitsandbytes llama-index-readers-web matplotlib flash-attn\n",
        "\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install --no-deps xformers peft trl"
      ],
      "metadata": {
        "id": "PXUvYSxCKTUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Important Libraries"
      ],
      "metadata": {
        "id": "hd97mwGxU5E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "import logging\n",
        "import sys\n",
        "from llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex,StorageContext\n",
        "from llama_index.core.graph_stores import SimpleGraphStore\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n"
      ],
      "metadata": {
        "id": "ocs8jLwYKRsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Load\n",
        "Loading text file having multiple paragraph"
      ],
      "metadata": {
        "id": "xpMIwRCkU9tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/data/\""
      ],
      "metadata": {
        "id": "Nzv849YqLkT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_parser = SentenceSplitter(chunk_size=128, chunk_overlap=20)\n",
        "documents = SimpleDirectoryReader(data_path,file_extractor=node_parser).load_data()\n",
        "nodes = node_parser.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "07PFUvmFKOn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nodes)"
      ],
      "metadata": {
        "id": "yapV34UvKara"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes[0].text\n"
      ],
      "metadata": {
        "id": "qYgodpfz359O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM & Embedding Settings"
      ],
      "metadata": {
        "id": "w3BcUFCoQ3g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name ='unsloth/Phi-3-mini-4k-instruct'\n",
        "embd_model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "model_kwargs={\"trust_remote_code\": True}\n",
        "generate_seq_args = {\"do_sample\": True, \"temperature\": 0.1}\n",
        "graph_store = SimpleGraphStore()\n",
        "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
      ],
      "metadata": {
        "id": "7f17jJw0Q3AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                                    model_name = model_name,\n",
        "                                    max_seq_length = max_seq_length,\n",
        "                                    dtype = dtype,\n",
        "                                    load_in_4bit = load_in_4bit)\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=embd_model_name)\n"
      ],
      "metadata": {
        "id": "y8hWn8IyQ6n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def messages_to_prompt(messages):\n",
        "  prompt = \"\"\n",
        "  system_found = False\n",
        "  for message in messages:\n",
        "      if message.role == \"system\":\n",
        "          prompt += f\"<|system|>\\n{message.content}<|end|>\\n\"\n",
        "          system_found = True\n",
        "      elif message.role == \"user\":\n",
        "          prompt += f\"<|user|>\\n{message.content}<|end|>\\n\"\n",
        "      elif message.role == \"assistant\":\n",
        "          prompt += f\"<|assistant|>\\n{message.content}<|end|>\\n\"\n",
        "      else:\n",
        "          prompt += f\"<|user|>\\n{message.content}<|end|>\\n\"\n",
        "\n",
        "  # trailing prompt\n",
        "  prompt += \"<|assistant|>\\n\"\n",
        "\n",
        "  if not system_found:\n",
        "      prompt = (\n",
        "          \"<|system|>\\nYou are a helpful AI assistant.<|end|>\\n\" + prompt\n",
        "      )\n",
        "\n",
        "  return prompt\n",
        "\n",
        "query_wrapper_prompt= (\n",
        "        \"<|system|>\\n\"\n",
        "        \"You are a helpful AI assistant, who is going to understand given knowledge graph. Your job is to understand thequery and write detailed answer<|end|>\\n\"\n",
        "        \"<|user|>\\n\"\n",
        "        \"{query_str}<|end|>\\n\"\n",
        "        \"<|assistant|>\\n\"\n",
        "    )\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "                          model=model,\n",
        "                          tokenizer=tokenizer,\n",
        "                          model_kwargs=model_kwargs,\n",
        "                          generate_kwargs= generate_seq_args,\n",
        "                          query_wrapper_prompt=query_wrapper_prompt,\n",
        "                          messages_to_prompt=messages_to_prompt,\n",
        "                          is_chat_model=True,\n",
        "                            )"
      ],
      "metadata": {
        "id": "2CnYuRjjRIJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Triplets by using seq-to-seq Model"
      ],
      "metadata": {
        "id": "MDH7Vq0sMXEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "bbl_tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
        "bbl_model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
        "gen_kwargs = {\n",
        "    \"max_length\": 256,\n",
        "    \"length_penalty\": 0,\n",
        "    \"num_beams\": 3,\n",
        "    \"num_return_sequences\": 3,\n",
        "}"
      ],
      "metadata": {
        "id": "4Z93J3UR4Rcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbl_model = bbl_model.to('cuda')\n"
      ],
      "metadata": {
        "id": "-B5M8CAMOomZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_to_seq_prediction(text):\n",
        "  model_inputs = bbl_tokenizer(text, max_length=256, padding=True, truncation=True, return_tensors = 'pt')\n",
        "\n",
        "  pred_tokens = bbl_model.generate(\n",
        "      model_inputs[\"input_ids\"].to(bbl_model.device),\n",
        "      attention_mask=model_inputs[\"attention_mask\"].to(bbl_model.device),\n",
        "      **gen_kwargs,\n",
        "  )\n",
        "\n",
        "  pred = bbl_tokenizer.batch_decode(pred_tokens, skip_special_tokens=False)\n",
        "  return(pred)\n"
      ],
      "metadata": {
        "id": "igEpJti3354Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_triplets(text):\n",
        "    triplets = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "    return triplets"
      ],
      "metadata": {
        "id": "KwBoVXXwNDDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbl_index = KnowledgeGraphIndex.from_documents([],include_embeddings=True)\n",
        "\n",
        "for idx, node in enumerate(nodes):\n",
        "  print(f'Processed Triplets for sentence {idx+1}')\n",
        "  pred = seq_to_seq_prediction(node.text)\n",
        "  for pr in pred:\n",
        "    triplet = extract_triplets(text = pr)\n",
        "    for trip in triplet:\n",
        "      kg_val = tuple(trip.values())\n",
        "      bbl_index.upsert_triplet_and_node(kg_val, nodes[idx])\n"
      ],
      "metadata": {
        "id": "FixnOjR_351f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_network(indx):\n",
        "  import networkx as nx\n",
        "  graph = indx.get_networkx_graph()\n",
        "  nx.draw(graph)\n"
      ],
      "metadata": {
        "id": "phN3j3BnP1U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_network(indx=bbl_index)"
      ],
      "metadata": {
        "id": "SDrRktoHNog1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbl_query_engine = bbl_index.as_query_engine(include_text=False, response_mode=\"tree_summarize\")"
      ],
      "metadata": {
        "id": "t8f2SToVN5WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0FdPqyySLm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing with Default Knowledge Graph Template of Lamma Index"
      ],
      "metadata": {
        "id": "EVyrrhAaLtUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_index = KnowledgeGraphIndex.from_documents(\n",
        "                  documents,\n",
        "                  max_triplets_per_chunk=2,\n",
        "                  storage_context=storage_context, include_embeddings=True\n",
        "                  )\n",
        "\n",
        "default_query_engine = default_index.as_query_engine(include_text=False, response_mode=\"tree_summarize\")"
      ],
      "metadata": {
        "id": "K0AH-VJA35sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get custom KG from LLM"
      ],
      "metadata": {
        "id": "x5KIptl3SZEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_KG_TRIPLET_EXTRACT_TMPL = (\n",
        "    \"You have been given the text below and you need to extract up to 3 \"\n",
        "    \"knowledge graph triplets in the form of (subject, predicate, object). You should not extract any puctuations.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Example:\"\n",
        "    \"Text: ITMS (15 micrograms.kg-1) was injected via standard dural puncture.\"\n",
        "    \"Triplets:\\n(ITMS, get injected, standard dural puncture)\\n\"\n",
        "    \"Text: Recurrent ulceration and mucosal tags are well-described oral manifestations of Crohn's disease.\\n\"\n",
        "    \"Triplets:\\n\"\n",
        "    \"(Recurrent ulceration, well-described, oral manifestations)\\n\"\n",
        "    \"(mucosal tags, well-described, oral manifestations)\\n\"\n",
        "    \"(oral manifestations, is related, Crohn's disease)\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Text: {text}\\n\"\n",
        "    \"Triplets:\\n\"\n",
        ")\n",
        "from llama_index.core.prompts.base import PromptTemplate\n",
        "from llama_index.core.prompts.prompt_type import PromptType\n",
        "\n",
        "DEFAULT_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n",
        "    DEFAULT_KG_TRIPLET_EXTRACT_TMPL,\n",
        "    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n",
        ")"
      ],
      "metadata": {
        "id": "xj8BySOtKu1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_prompt_triplets(text):\n",
        "  prompt = DEFAULT_KG_TRIPLET_EXTRACT_PROMPT.format(text=text)\n",
        "  response = Settings.llm.complete(prompt)\n",
        "  res = response.text.split('\\n')\n",
        "  # triplets = []\n",
        "  # for ans in res:\n",
        "  #   out = [an.replace('(','').replace(')','').strip() for an in ans.split(',')]\n",
        "  #   triplets.append(out)\n",
        "  return(res)"
      ],
      "metadata": {
        "id": "Hsmxsdm2KElX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kg_val"
      ],
      "metadata": {
        "id": "RzVA_8lAhnFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_index = KnowledgeGraphIndex.from_documents([],include_embeddings=True)\n",
        "\n",
        "for idx, node in enumerate(nodes):\n",
        "  print(f'Processed Triplets for sentence {idx+1}')\n",
        "  response = extract_prompt_triplets(text=node.text)\n",
        "  for trip in response:\n",
        "    kg_val = [one_entity.replace('(','').replace(')','').strip() for one_entity in trip.split(',')][:3]\n",
        "    if len(kg_val)==3:\n",
        "      custom_index.upsert_triplet_and_node(kg_val, nodes[idx])\n"
      ],
      "metadata": {
        "id": "1i91AiV4J-Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_query_engine = custom_index.as_query_engine(include_text=False, response_mode=\"tree_summarize\")"
      ],
      "metadata": {
        "id": "BkmyWAqOJ-Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkEEmB9DJ9__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_response(response):\n",
        "    return(display(Markdown(f\"<b>{response}</b>\")))"
      ],
      "metadata": {
        "id": "4qKL44PrLdMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = default_query_engine.query(\n",
        "    \"What are the causing factor for unilateral facial weakness?\",\n",
        ")\n",
        "format_response(response)"
      ],
      "metadata": {
        "id": "FbwCTGQsEvWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.source_nodes[0].metadata['kg_rel_texts']"
      ],
      "metadata": {
        "id": "MAnE_tYqS9yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = bbl_query_engine.query(\n",
        "    \"What are the causing factor for unilateral facial weakness?\",\n",
        ")\n",
        "format_response(response)"
      ],
      "metadata": {
        "id": "gsxbcDzpxeV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.source_nodes[0].metadata['kg_rel_texts']"
      ],
      "metadata": {
        "id": "9ztiymZUxgnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = custom_query_engine.query(\n",
        "    \"What are the causing factor for unilateral facial weakness?\",\n",
        ")\n",
        "format_response(response)"
      ],
      "metadata": {
        "id": "41ypIxg4MNwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.source_nodes[0].metadata['kg_rel_texts']"
      ],
      "metadata": {
        "id": "Dm0nxPEtksyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYPh6TF2UwsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbTwo1E6UZRi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}